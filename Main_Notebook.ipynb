{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Outbrain Click Prediction\n",
    "\n",
    "## 1. Data Preparation and Cleaning\n",
    "\n",
    "### Import Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as p\n",
    "import pickle\n",
    "import zipfile\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get metadata and functions utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = os.getcwd() + \"/\"\n",
    "utils_path = path + \"utils/\"\n",
    "meta = utils_path + \"meta.py\"\n",
    "functions = utils_path + \"functions.py\"\n",
    "%run $meta\n",
    "%run $functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unzip source tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "zip_ref = zipfile.ZipFile(data_tables, 'r')\n",
    "zip_ref.extractall(tables_path)\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Train & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   display_id   ad_id clicked\n",
      "0           1   42337   False\n",
      "1           1  139684   False\n",
      "2           1  144739    True\n",
      "3           1  156824   False\n",
      "4           1  279295   False\n"
     ]
    }
   ],
   "source": [
    "train = p.read_csv(tables_path + \"train.csv\")\n",
    "test = p.read_csv(tables_path + \"test.csv\")\n",
    "\n",
    "print (train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choose \"fraction\" of train & test for training  & predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1311190, 3)\n",
      "(433480, 3)\n"
     ]
    }
   ],
   "source": [
    "#set size of fraction to take from train and test, for full tables, fraction = 0\n",
    "fraction = 0.1\n",
    "\n",
    "if fraction:\n",
    "    train, test = fractioned(train,test,fraction)\n",
    "    print (train.shape)\n",
    "    print (test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare platform table\n",
    "#### 1. Impute missing values with median\n",
    "#### 2. Create one-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   display_id  document_id  plat_1  plat_2  plat_3\n",
      "0           1       379743       0       0       1\n",
      "1           2      1794259       0       1       0\n",
      "2           3      1179111       0       1       0\n",
      "3           4      1777797       0       1       0\n",
      "4           5       252458       0       1       0\n"
     ]
    }
   ],
   "source": [
    "skip_computation = True\n",
    "\n",
    "if (not skip_computation):\n",
    "    platform_prep = features_path + \"platform_prep.py\"\n",
    "    %run $platform_prep\n",
    "\n",
    "platform = p.read_csv(tables_path + \"platform.csv\")\n",
    "print (platform.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare topics & categories table\n",
    "#### 1. Merge categories, topics tables\n",
    "#### 2. For each document - leave only topic/category with the highest confidence level\n",
    "#### 3. Impute missing documents with value -1 (assign confidence 0) and cast id's back to integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   document_id  topic_id  confi_top  category_id  confi_cat\n",
      "0      1595802       140   0.073113         1611   0.920000\n",
      "1      1524246       113   0.196450         1807   0.920000\n",
      "2      1617787       113   0.216892         1807   0.920000\n",
      "3      1615583        89   0.316306         1305   0.920000\n",
      "4      1615460       260   0.097964         1613   0.540646\n"
     ]
    }
   ],
   "source": [
    "skip_computation = True\n",
    "\n",
    "if (not skip_computation):\n",
    "    topics_categories_prep = features_path + \"topics_categories_prep.py\"\n",
    "    %run $topics_categories_prep\n",
    "    \n",
    "topics_categories = p.read_csv(tables_path + \"topics_categories.csv\")\n",
    "print (topics_categories.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Promoted Content table\n",
    "#### 1. Merge with topics_categories table\n",
    "#### 2. Impute missing documents with -1 (assign confidence 0) and cast id's back to integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ad_id  ad_document_id  campaign_id  advertiser_id  topic_id  confi_top  \\\n",
      "0      1            6614            1              7        -1   0.000000   \n",
      "1      2          471467            2              7        89   0.122998   \n",
      "2      3            7692            3              7        26   0.104300   \n",
      "3      4          471471            2              7       168   0.076367   \n",
      "4      5          471472            2              7       143   0.089059   \n",
      "\n",
      "   category_id  confi_cat  \n",
      "0         1209   0.925264  \n",
      "1         1505   0.920000  \n",
      "2         1209   0.920000  \n",
      "3         1205   0.920000  \n",
      "4         1608   0.920000  \n"
     ]
    }
   ],
   "source": [
    "skip_computation = True\n",
    "\n",
    "if (not skip_computation):\n",
    "    promoted_prep = features_path + \"promoted_content_prep.py\"\n",
    "    %run $promoted_prep\n",
    "\n",
    "promoted = p.read_csv(tables_path + \"promoted_content_prep.csv\")\n",
    "print (promoted.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge all tables with train & test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   display_id   ad_id clicked  document_id  plat_1  plat_2  plat_3\n",
      "0     1409220   26722   False      1049648       0       0       1\n",
      "1     1409220   39675   False      1049648       0       0       1\n",
      "2     1409220  129490    True      1049648       0       0       1\n",
      "3     1409220  144096   False      1049648       0       0       1\n",
      "4     1409220  179041   False      1049648       0       0       1\n"
     ]
    }
   ],
   "source": [
    "train = train.merge(platform, how='left', on='display_id')\n",
    "test = test.merge(platform, how='left', on='display_id')\n",
    "del platform\n",
    "\n",
    "print (train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   display_id   ad_id clicked  document_id  plat_1  plat_2  plat_3  \\\n",
      "0     1409220   26722   False      1049648       0       0       1   \n",
      "1     1409220   39675   False      1049648       0       0       1   \n",
      "2     1409220  129490    True      1049648       0       0       1   \n",
      "3     1409220  144096   False      1049648       0       0       1   \n",
      "4     1409220  179041   False      1049648       0       0       1   \n",
      "\n",
      "   ad_document_id  campaign_id  advertiser_id  topic_id  confi_top  \\\n",
      "0          840223         4035           1299       160   0.428540   \n",
      "1          700044         2491           1299       198   0.323995   \n",
      "2         1060089         9756           1593       227   0.334232   \n",
      "3         1333780          606             90       151   0.076847   \n",
      "4         1351158        21726           3190       138   0.065299   \n",
      "\n",
      "   category_id  confi_cat  \n",
      "0         1403   0.920000  \n",
      "1         1403   0.920000  \n",
      "2         1403   0.920000  \n",
      "3         1210   0.447978  \n",
      "4         1403   0.440432  \n"
     ]
    }
   ],
   "source": [
    "train = train.merge(promoted,how = 'left',on = 'ad_id')\n",
    "test = test.merge(promoted,how = 'left',on = 'ad_id')\n",
    "del promoted\n",
    "\n",
    "print (train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   display_id   ad_id clicked  document_id  plat_1  plat_2  plat_3  \\\n",
      "0     1409220   26722   False      1049648       0       0       1   \n",
      "1     1409220   39675   False      1049648       0       0       1   \n",
      "2     1409220  129490    True      1049648       0       0       1   \n",
      "3     1409220  144096   False      1049648       0       0       1   \n",
      "4     1409220  179041   False      1049648       0       0       1   \n",
      "\n",
      "   ad_document_id  campaign_id  advertiser_id  topic_id_ad  confi_top_ad  \\\n",
      "0          840223         4035           1299          160      0.428540   \n",
      "1          700044         2491           1299          198      0.323995   \n",
      "2         1060089         9756           1593          227      0.334232   \n",
      "3         1333780          606             90          151      0.076847   \n",
      "4         1351158        21726           3190          138      0.065299   \n",
      "\n",
      "   category_id_ad  confi_cat_ad  topic_id_doc  confi_top_doc  category_id_doc  \\\n",
      "0            1403      0.920000         265.0       0.116894           1912.0   \n",
      "1            1403      0.920000         265.0       0.116894           1912.0   \n",
      "2            1403      0.920000         265.0       0.116894           1912.0   \n",
      "3            1210      0.447978         265.0       0.116894           1912.0   \n",
      "4            1403      0.440432         265.0       0.116894           1912.0   \n",
      "\n",
      "   confi_cat_doc  \n",
      "0       0.745302  \n",
      "1       0.745302  \n",
      "2       0.745302  \n",
      "3       0.745302  \n",
      "4       0.745302  \n"
     ]
    }
   ],
   "source": [
    "train = train.merge(topics_categories, how='left', on='document_id',suffixes=('_ad', '_doc'))\n",
    "test = test.merge(topics_categories, how='left', on='document_id',suffixes=('_ad', '_doc'))\n",
    "del topics_categories\n",
    "\n",
    "print (train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Impute missing topics/categories with value -1 (assign confidence 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Missing Values  % of Total Values\n",
      "topic_id_doc              31587           2.409033\n",
      "confi_top_doc             31587           2.409033\n",
      "category_id_doc           31587           2.409033\n",
      "confi_cat_doc             31587           2.409033\n"
     ]
    }
   ],
   "source": [
    "missing_values = missing_values_table(train,1)\n",
    "print (missing_values[missing_values[\"% of Total Values\"] != 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train.confi_top_doc = train.confi_top_doc.fillna(0)\n",
    "train.confi_cat_doc = train.confi_cat_doc.fillna(0)\n",
    "test.confi_top_doc = test.confi_top_doc.fillna(0)\n",
    "test.confi_cat_doc = test.confi_cat_doc.fillna(0)\n",
    "train = train.fillna(-1)\n",
    "test = test.fillna(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add boolean for identity between ad and document topic & ad and document category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shared_subjects(train)\n",
    "shared_subjects(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   display_id   ad_id clicked  document_id  plat_1  plat_2  plat_3  \\\n",
      "0     1409220   26722   False      1049648       0       0       1   \n",
      "1     1409220   39675   False      1049648       0       0       1   \n",
      "2     1409220  129490    True      1049648       0       0       1   \n",
      "3     1409220  144096   False      1049648       0       0       1   \n",
      "4     1409220  179041   False      1049648       0       0       1   \n",
      "\n",
      "   ad_document_id  campaign_id  advertiser_id      ...        \\\n",
      "0          840223         4035           1299      ...         \n",
      "1          700044         2491           1299      ...         \n",
      "2         1060089         9756           1593      ...         \n",
      "3         1333780          606             90      ...         \n",
      "4         1351158        21726           3190      ...         \n",
      "\n",
      "   score_docXad_doc  score_docXadv  score_docXcamp  weekend  morning  noon  \\\n",
      "0          1.100718       1.514402        1.106262        0        0     0   \n",
      "1          1.673785       1.514402        1.678247        0        0     0   \n",
      "2          0.999217       1.023067        1.002104        0        0     0   \n",
      "3          1.575921       1.623828        1.581720        0        0     0   \n",
      "4          1.232581       1.146237        1.237421        0        0     0   \n",
      "\n",
      "   evening  night  same_topic  same_category  \n",
      "0        1      0       False          False  \n",
      "1        1      0       False          False  \n",
      "2        1      0       False          False  \n",
      "3        1      0       False          False  \n",
      "4        1      0       False          False  \n",
      "\n",
      "[5 rows x 35 columns]\n"
     ]
    }
   ],
   "source": [
    "print (train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#export end of data preparation and cleaning\n",
    "train.to_csv(tables_path + 'train_prep.csv', index=False)\n",
    "test.to_csv(tables_path + 'test_prep.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering\n",
    "### 2.1. Click Through Rate\n",
    "### See feature engineering section in documetation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "skip_computation = True\n",
    "\n",
    "if (skip_computation):\n",
    "    zip_ref = zipfile.ZipFile(ctr_tables, 'r')\n",
    "    zip_ref.extractall(tables_path)\n",
    "    zip_ref.close()\n",
    "\n",
    "else:\n",
    "    ctr_features = features_path + \"ctr_features.py\"\n",
    "    %run $ctr_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 topics & categories correlations\n",
    "#### 1. Build dictionaries holding for each pair (topic, topic) or (category, category) the strength of their correlation (more on documentaion)\n",
    "#### 2. confidence_cut_category, confidence_cut_topic, score_cut_category, score_cut_topic determines min thresholds for confidence level and correlation score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "skip_computation = True\n",
    "\n",
    "if (not skip_computation):\n",
    "    dictionaries_prep = features_path + \"dictionaries_prep.py\"\n",
    "    %run $dictionaries_prep\n",
    "    \n",
    "    #threshold confidence level to keep\n",
    "    confidence_cut_category = 0.5  \n",
    "    confidence_cut_topic = 0.1\n",
    "    \n",
    "    #which portion of top scores to take, takes 1/parameter highest scores\n",
    "    score_cut_category = 4\n",
    "    score_cut_topic = 4\n",
    "    \n",
    "    category_dict_name, topic_dict_name = create_dicts(confidence_cut_category, confidence_cut_topic, \n",
    "                                                      score_cut_category, score_cut_topic)\n",
    "\n",
    "else:    \n",
    "    topic_dict_name = default_topic_dict_name\n",
    "    category_dict_name = default_category_dict_name\n",
    "\n",
    "\n",
    "#load dictionaries for correlation of topics and categories\n",
    "with open(dicts_path + topic_dict_name, 'rb') as handle:\n",
    "    top_dict = pickle.load(handle, encoding='iso-8859-1')\n",
    "    \n",
    "with open(dicts_path + category_dict_name, 'rb') as handle:\n",
    "    cat_dict = pickle.load(handle, encoding='iso-8859-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Fill NA values with median correlation\n",
    "#### 3. Merge correlations values with train & test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, test = correlations(train, test, top_dict, cat_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2.3. Click Time\n",
    "### Requires packages: pygecoders, geopy, pycountry, pytz, tzwhere, shapley\n",
    "#### 1. Parse geo_location to country and state\n",
    "#### 2. For each location compute offset from UTC\n",
    "#### 3. Fill NA with offset 0\n",
    "#### 4. Add offset to timestamp\n",
    "#### 5. Convert timestamp to one-hot of: { morning, noon, evening, night, weekend }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   display_id   ad_id clicked  document_id  plat_1  plat_2  plat_3  \\\n",
      "0     1409220   26722   False      1049648       0       0       1   \n",
      "1     1409220   39675   False      1049648       0       0       1   \n",
      "2     1409220  129490    True      1049648       0       0       1   \n",
      "3     1409220  144096   False      1049648       0       0       1   \n",
      "4     1409220  179041   False      1049648       0       0       1   \n",
      "\n",
      "   ad_document_id  campaign_id  advertiser_id  topic_id_ad  confi_top_ad  \\\n",
      "0          840223         4035           1299          160      0.428540   \n",
      "1          700044         2491           1299          198      0.323995   \n",
      "2         1060089         9756           1593          227      0.334232   \n",
      "3         1333780          606             90          151      0.076847   \n",
      "4         1351158        21726           3190          138      0.065299   \n",
      "\n",
      "   category_id_ad  confi_cat_ad  topic_id_doc  confi_top_doc  category_id_doc  \\\n",
      "0            1403      0.920000         265.0       0.116894           1912.0   \n",
      "1            1403      0.920000         265.0       0.116894           1912.0   \n",
      "2            1403      0.920000         265.0       0.116894           1912.0   \n",
      "3            1210      0.447978         265.0       0.116894           1912.0   \n",
      "4            1403      0.440432         265.0       0.116894           1912.0   \n",
      "\n",
      "   confi_cat_doc   cor_top   cor_cat  \n",
      "0       0.745302  0.009336  1.056498  \n",
      "1       0.745302  0.006586  1.056498  \n",
      "2       0.745302  0.007801  1.056498  \n",
      "3       0.745302  0.001573  0.493635  \n",
      "4       0.745302  0.001369  0.505777  \n"
     ]
    }
   ],
   "source": [
    "print (train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   display_id  weekend  morning  noon  evening  night\n",
      "0           1        0        0     0        1      0\n",
      "1           2        0        0     1        0      0\n",
      "2           3        0        0     0        1      0\n",
      "3           4        0        0     0        1      0\n",
      "4           5        0        1     0        0      0\n"
     ]
    }
   ],
   "source": [
    "skip_computation = True\n",
    "\n",
    "if (not skip_computation):\n",
    "    timezone = features_path + \"timezone.py\"\n",
    "    %run $timezone\n",
    "    \n",
    "time_table = p.read_csv(tables_path + \"time_table.csv\")\n",
    "print (time_table.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge features with train & test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train, test = merge_ctrs_and_time(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   display_id   ad_id clicked  document_id  plat_1  plat_2  plat_3  \\\n",
      "0     1409220   26722   False      1049648       0       0       1   \n",
      "1     1409220   39675   False      1049648       0       0       1   \n",
      "2     1409220  129490    True      1049648       0       0       1   \n",
      "3     1409220  144096   False      1049648       0       0       1   \n",
      "4     1409220  179041   False      1049648       0       0       1   \n",
      "\n",
      "   ad_document_id  campaign_id  advertiser_id      ...        \\\n",
      "0          840223         4035           1299      ...         \n",
      "1          700044         2491           1299      ...         \n",
      "2         1060089         9756           1593      ...         \n",
      "3         1333780          606             90      ...         \n",
      "4         1351158        21726           3190      ...         \n",
      "\n",
      "   score_docXad_doc  score_docXadv  score_docXcamp  weekend  morning  noon  \\\n",
      "0          1.100718       1.514402        1.106262        0        0     0   \n",
      "1          1.673785       1.514402        1.678247        0        0     0   \n",
      "2          0.999217       1.023067        1.002104        0        0     0   \n",
      "3          1.575921       1.623828        1.581720        0        0     0   \n",
      "4          1.232581       1.146237        1.237421        0        0     0   \n",
      "\n",
      "   evening  night  same_topic  same_category  \n",
      "0        1      0       False          False  \n",
      "1        1      0       False          False  \n",
      "2        1      0       False          False  \n",
      "3        1      0       False          False  \n",
      "4        1      0       False          False  \n",
      "\n",
      "[5 rows x 35 columns]\n"
     ]
    }
   ],
   "source": [
    "print (train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save final train & test\n",
    "train.to_csv(tables_path + 'train_features.csv', index=False)\n",
    "test.to_csv(tables_path + 'test_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load final train & test - for repeated runs\n",
    "train = p.read_csv(path_b + 'train_features.csv')\n",
    "test = p.read_csv(path_b + 'test_features.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### all pairs of display documents and ad attributes has extremely high rate of nulls\n",
    "### Impute missing values with mean values or median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Missing Values  % of Total Values\n",
      "score_ad                   16561           3.820476\n",
      "score_ad_doc                8684           2.003322\n",
      "score_adv                   1052           0.242687\n",
      "score_camp                  4266           0.984128\n",
      "score_docXad              264438          61.003507\n",
      "score_docXad_doc          241571          55.728292\n",
      "score_docXadv             203941          47.047384\n",
      "score_docXcamp            237292          54.741165\n"
     ]
    }
   ],
   "source": [
    "missing_values = missing_values_table(test, 0)\n",
    "print (missing_values[missing_values[\"% of Total Values\"] != 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#choose \"median\" or \"mean\"\n",
    "fill = \"median\"\n",
    "fill_na(test,fill)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3. Model Selection\n",
    "### 1. Split train to train and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train, validation = train_validation_split(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. set predictors for model comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictors= [x for x in train.columns if x not in \n",
    "             ['display_id','ad_id','clicked','document_id','ad_document_id',\n",
    "              'campaign_id','advertiser_id','confi_top_ad','topic_id_ad','topic_id_doc',\n",
    "              'category_id_ad','confi_cat_ad','confi_top_doc','category_id_doc','confi_cat_doc']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>plat_1</th>\n",
       "      <th>plat_2</th>\n",
       "      <th>plat_3</th>\n",
       "      <th>cor_top</th>\n",
       "      <th>cor_cat</th>\n",
       "      <th>score_ad</th>\n",
       "      <th>score_ad_doc</th>\n",
       "      <th>score_adv</th>\n",
       "      <th>score_camp</th>\n",
       "      <th>score_docXad</th>\n",
       "      <th>score_docXad_doc</th>\n",
       "      <th>score_docXadv</th>\n",
       "      <th>score_docXcamp</th>\n",
       "      <th>weekend</th>\n",
       "      <th>morning</th>\n",
       "      <th>noon</th>\n",
       "      <th>evening</th>\n",
       "      <th>night</th>\n",
       "      <th>same_topic</th>\n",
       "      <th>same_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.009336</td>\n",
       "      <td>1.056498</td>\n",
       "      <td>1.077518</td>\n",
       "      <td>1.059151</td>\n",
       "      <td>1.147862</td>\n",
       "      <td>1.060096</td>\n",
       "      <td>1.200817</td>\n",
       "      <td>1.100718</td>\n",
       "      <td>1.514402</td>\n",
       "      <td>1.106262</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.006586</td>\n",
       "      <td>1.056498</td>\n",
       "      <td>1.146762</td>\n",
       "      <td>1.139692</td>\n",
       "      <td>1.147862</td>\n",
       "      <td>1.140961</td>\n",
       "      <td>1.662041</td>\n",
       "      <td>1.673785</td>\n",
       "      <td>1.514402</td>\n",
       "      <td>1.678247</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.007801</td>\n",
       "      <td>1.056498</td>\n",
       "      <td>0.719273</td>\n",
       "      <td>0.662975</td>\n",
       "      <td>0.626853</td>\n",
       "      <td>0.721160</td>\n",
       "      <td>0.991616</td>\n",
       "      <td>0.999217</td>\n",
       "      <td>1.023067</td>\n",
       "      <td>1.002104</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001573</td>\n",
       "      <td>0.493635</td>\n",
       "      <td>1.350336</td>\n",
       "      <td>1.350924</td>\n",
       "      <td>1.568107</td>\n",
       "      <td>1.302799</td>\n",
       "      <td>1.560653</td>\n",
       "      <td>1.575921</td>\n",
       "      <td>1.623828</td>\n",
       "      <td>1.581720</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001369</td>\n",
       "      <td>0.505777</td>\n",
       "      <td>1.287411</td>\n",
       "      <td>1.215748</td>\n",
       "      <td>1.085697</td>\n",
       "      <td>1.218249</td>\n",
       "      <td>1.219842</td>\n",
       "      <td>1.232581</td>\n",
       "      <td>1.146237</td>\n",
       "      <td>1.237421</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   plat_1  plat_2  plat_3   cor_top   cor_cat  score_ad  score_ad_doc  \\\n",
       "0       0       0       1  0.009336  1.056498  1.077518      1.059151   \n",
       "1       0       0       1  0.006586  1.056498  1.146762      1.139692   \n",
       "2       0       0       1  0.007801  1.056498  0.719273      0.662975   \n",
       "3       0       0       1  0.001573  0.493635  1.350336      1.350924   \n",
       "4       0       0       1  0.001369  0.505777  1.287411      1.215748   \n",
       "\n",
       "   score_adv  score_camp  score_docXad  score_docXad_doc  score_docXadv  \\\n",
       "0   1.147862    1.060096      1.200817          1.100718       1.514402   \n",
       "1   1.147862    1.140961      1.662041          1.673785       1.514402   \n",
       "2   0.626853    0.721160      0.991616          0.999217       1.023067   \n",
       "3   1.568107    1.302799      1.560653          1.575921       1.623828   \n",
       "4   1.085697    1.218249      1.219842          1.232581       1.146237   \n",
       "\n",
       "   score_docXcamp  weekend  morning  noon  evening  night same_topic  \\\n",
       "0        1.106262        0        0     0        1      0      False   \n",
       "1        1.678247        0        0     0        1      0      False   \n",
       "2        1.002104        0        0     0        1      0      False   \n",
       "3        1.581720        0        0     0        1      0      False   \n",
       "4        1.237421        0        0     0        1      0      False   \n",
       "\n",
       "  same_category  \n",
       "0         False  \n",
       "1         False  \n",
       "2         False  \n",
       "3         False  \n",
       "4         False  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[predictors].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Logistic Regression with different combinations of features to get the best predictors\n",
    "#### Logistic Regression was chosen because this computation takes long time, and it is the fastest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score is: 0.74708156797344183\n"
     ]
    }
   ],
   "source": [
    "predictors = feature_selection(train, validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['plat_1', 'plat_2', 'plat_3', 'cor_cat', 'score_ad', 'score_ad_doc', 'score_adv', 'score_camp', 'score_docXad', 'score_docXad_doc', 'score_docXadv', 'score_docXcamp', 'same_topic', 'same_category']\n"
     ]
    }
   ],
   "source": [
    "print(predictors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Compare between Random Forest, Gradient Boost, Logistic Regression\n",
    "#### Parameters Tuning - for each model run grid search on 2 selected parameters to get best values\n",
    "#### Evaluate model by computing MAP @12 score on prediction made on validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters: min_samples_split = 2, n_estimators = 10, score: 0.78980125798627487\n",
      "parameters: min_samples_split = 4, n_estimators = 10, score: 0.79377628264884736\n",
      "parameters: min_samples_split = 2, n_estimators = 25, score: 0.80211288953850501\n",
      "parameters: min_samples_split = 4, n_estimators = 25, score: 0.80561643390478666\n",
      "best parameters: min_samples_split = 4, n_estimators = 25, score: 0.80561643390478666\n"
     ]
    }
   ],
   "source": [
    "#Random Forest\n",
    "min_sample_split = [2,4]\n",
    "n_estimators = [10, 25]\n",
    "forest = grid_search(\"randomforest\", min_sample_split, n_estimators, train, validation, predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters: learning_rate = 0.05, max_depth = 2, score: 0.82479658610921103\n",
      "parameters: learning_rate = 0.1, max_depth = 2, score: 0.83340222588290447\n",
      "parameters: learning_rate = 0.05, max_depth = 4, score: 0.83893429556470611\n",
      "parameters: learning_rate = 0.1, max_depth = 4, score: 0.84261499589162014\n",
      "best parameters: learning_rate = 0.1, max_depth = 4, score: 0.84261499589162014\n"
     ]
    }
   ],
   "source": [
    "#Gradiant Boosting\n",
    "learning_rate = [0.05,0.1]\n",
    "max_depth = [2,4]\n",
    "gradient = grid_search(\"gradient\", learning_rate, max_depth, train, validation, predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters: C = 1e-05, solver = 'sag', score: 0.74706545940403568\n",
      "parameters: C = 1e-09, solver = 'sag', score: 0.73966907846262897\n",
      "parameters: C = 1e-05, solver = 'lbfgs', score: 0.74705311342018421\n",
      "parameters: C = 1e-09, solver = 'lbfgs', score: 0.73966907846262897\n",
      "best parameters: C = 1e-05, solver = 'sag', score: 0.74706545940403568\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression\n",
    "C = [0.00001,0.000000001]\n",
    "solver = [\"sag\", \"lbfgs\"]\n",
    "logistic = grid_search(\"logistic\", C, solver, train, validation, predictors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Make prediction on test with all models using selected features and optimal parameters found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP: 0.574430357937\n",
      "PORTION: 0.348629257431\n",
      "MAP: 0.617754417959\n",
      "PORTION: 0.401143373997\n",
      "MAP: 0.649552832968\n",
      "PORTION: 0.445617652636\n"
     ]
    }
   ],
   "source": [
    "for model in [forest, gradient, logistic]:\n",
    "    alg = model[0]\n",
    "    alg.fit(train[predictors], train[\"clicked\"])\n",
    "    predict(alg, test, predictors)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
